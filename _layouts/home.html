<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->

<head>
{% include _head.html %}
</head>

<body class="home">

<div id="main" role="main">
  <div class="article-author-side">
    {% include _author-bio.html %}
  </div>
  <article class="page">
    <!--<h1>{{site.title}}</h1>-->
    {{content}}
    <hr>

    <!--
          <center>
          <iframe src="https://openprocessing.org/sketch/2054910/embed/?plusEmbedHash=b9e5cd59&userID=318845&plusEmbedTitle=false#sketch" style="width:80%; height:50px;"></iframe>
        </center>
    -->
    <p>My research is at the intersection of statistics and medicine. More specific topics are listed below.</p>
    <p>Medical decision making / decision theory / reinforcement learning</p> 
      <ul>
  <li><a href="https://onlinelibrary.wiley.com/doi/full/10.1002/sim.9755">Relative sparsity for medical decision problems</a></li>
    <li>Inference for relative sparsity (<a href="https://arxiv.org/pdf/2306.14297.pdf">preprint</a>)</li>
      </ul>

    <p>Environmental health / inverse reinforcement learning</p>
      <ul>
  <li>Optimality-based reward learning with applications to toxicology (<a href="https://arxiv.org/pdf/2404.04406.pdf">preprint</a>)</li>
      </ul>
    
    <p>Prediction</p>
    <ul>
       <li><a href="https://pubmed.ncbi.nlm.nih.gov/30458044/">Predicting AKI with EHR data</a></li>
 <li>On calibration (<a href="https://stats.stackexchange.com/questions/263393/scikit-correct-way-to-calibrate-classifiers-with-calibratedclassifiercv/288470#288470">forum answer</a>)</li>
    </ul>

    <p>Radiation dose monitoring / quality control / outlier detection</p>
<ul>
  <li><a href="https://pubmed.ncbi.nlm.nih.gov/26644157/">Radiation Exposure Extraction Engine</a></li>
</ul>

<p>Diagnosis / Covid-19 / large language models </p>
<ul>
  <li>ChatGPT and post-test probability (<a href="https://arxiv.org/pdf/2311.12188.pdf">preprint</a>)</li>
         <li>On the post-test probability of covid-19 (<a href="https://github.com/samuelweisenthal/BinaxNOWpostTestProb/blob/main/covid.post.test.pdf">blog post</a>)</li>
       </ul>

        <p>Educational talks</p>
    <ul>
          <li>On prediction (<a href="https://discourse.datamethods.org/t/what-should-mds-in-training-know-about-medical-prediction/335">forum question</a>)</li>
          <li><a href="https://docs.google.com/presentation/d/1xbUmjJ5iFcl16fFCQeqJVOVObIK8NKm0FgxPjGTnnQ8/edit#slide=id.p">Risk calculator lecture</a></li>
    </ul>
    
    
    <p>A full list of my publications can be found <a href="https://scholar.google.com/citations?hl=en&user=bbcNyWwAAAAJ&view_op=list_works&sortby=pubdate">here</a>.</p>
    <!--
    <p>Online discussions include
      <ul>
   <li><a href="https://stats.stackexchange.com/questions/295383/why-is-data-augmentation-classified-as-a-type-of-regularization/295394#295394">On data augmentation as regularization</a></li>     
      </ul>
    </p>
    -->
           <p>Thoughts</p>
    <ul>
          <li>I try to be mindful about not anthropomorphizing statistical models. For example, when working on a method called “... - based learning,” I wanted to suggest calling it “... - aware learning,” but I stopped myself.  Anthropomorphism is great because it helps people quickly understand what you’re doing; it’s bad because it can be misleading. One needs to always consider whether the cons of being misleading outweigh the pros of quicker understanding.</li>
          <li>Consider a guidline that administers home supplemental O2 if O2 sat is less than 88%. Putting this in the language of decision analysis, this treatment policy is pi(Supp O2=1|ox sat) = I(ox sat<88%). Is pi=I(ox sat<88%) really optimal for someone who has O2 sat 88.1%?</li>
    <li>I don’t like: sensitivity = TP/(TP + FN). I do like: sensitivity estimates p(Test+|Dz+).</li>
          <li>A black box predictive model can be useful in the clinic, if it is revalidated, and possibly retrained, everywhere it is used. Better understanding of a risk factor, however, can be communicated instantly, for free, around the world. Model interpretability is important.</li>
          <li>When trying to understand a mathematical proof or argument, I often use “backtracking” and “momentum.”  Consider a proof outline

Axiom
Statement 1
Statement 2
Statement 3

            If I am stuck on Statement 2, I don’t usually stay there for long. Instead, I backtrack to Statement 1. After having ensured that I do - truly - accept Statement 1, I then return to Statement 2. I often then do better with Statement 2.
          Backtracking might sound like common sense, but I think there is tendency to instead linger where we get stuck.
            Momentum:

If I get stuck at Statement 2, rather than backtracking to Statement 1, I often backtrack all the way to Axiom. Then I run through Axiom, Statement 1, Statement 2. This feels to me like building up momentum, increasing the chances that I break through Statement 2.
            I hope that backtracking and momentum might be useful to you someday as well!
          </li>
          
          </ul>

    
    
  </article>
</div><!-- /#main -->

{% include _scripts.html %}

</body>
</html>
